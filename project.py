import streamlit as st
from openai import OpenAI
import scipy.io.wavfile as wav
import tempfile
import sounddevice as sd
import os
import shutil
import cv2
import av
from keras.models import load_model
from PIL import Image, ImageOps
from streamlit_webrtc import webrtc_streamer, WebRtcMode
import numpy as np
import random
from dotenv import load_dotenv 
load_dotenv() 

# -----------------------------
# ì„¤ì •ê°’
# -----------------------------
sd.default.device = 1
SAMPLERATE = 48000
DURATION = 3

# ì´ë¯¸ ì„ ì–¸í•œ client ì‚¬ìš© (í‚¤ê°€ ì½”ë“œì— ë“¤ì–´ìˆë‹¤ê³  ê°€ì •)
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or st.secrets['OPENAI_API_KEY']
client = OpenAI(api_key=OPENAI_API_KEY)
# -----------------------------
# ìœ í‹¸ í•¨ìˆ˜ë“¤
# -----------------------------
def change_panel(goto: int):
    if goto == 0:
        st.session_state.panel = "voice_select"
        st.rerun()
    elif goto == 1:
        st.session_state.panel = "memo"
        st.rerun()
    elif goto == 2:
        st.session_state.panel = "hand_lang"
        st.rerun()


def record_audio():
    print("ğŸ™ï¸ ë…¹ìŒ ì¤‘...")
    audio = sd.rec(int(SAMPLERATE * DURATION), samplerate=SAMPLERATE, channels=1, dtype="int16")
    sd.wait()
    return audio

def transcribe(audio_data):
    # ë…¹ìŒ ë°ì´í„°ë¥¼ ì„ì‹œ wavë¡œ ì €ì¥ â†’ Whisper(Transcribe) í˜¸ì¶œ â†’ í…ìŠ¤íŠ¸ ë°˜í™˜
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_wav:
        wav.write(temp_wav.name, SAMPLERATE, audio_data)
        temp_wav_path = temp_wav.name

    try:
        with open(temp_wav_path, "rb") as file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=file,
                language="ko"
            )
        return transcript.text
    finally:
        try:
            os.remove(temp_wav_path)
        except:
            pass

def text_to_speech(text: str, voicec: str = "alloy") -> str:
    """
    text -> mp3 íŒŒì¼ ìƒì„± í›„ íŒŒì¼ ê²½ë¡œ ë°˜í™˜.
    voice: OpenAI ìŒì„± ì´ë¦„ (ì˜ˆ: "alloy", "fable", "echo", "nova", "shimmer" ë“±)
    ë°˜í™˜ê°’: ìƒì„±ëœ mp3 íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ
    """
    # ì„ì‹œ íŒŒì¼ ìƒì„±
    out_dir = tempfile.mkdtemp(prefix="tts_out_")
    out_path = os.path.join(out_dir, "speech.mp3")

    try:
        # OpenAI TTS í˜¸ì¶œ (SDKì˜ stream_to_file ì´ìš©)
        # response = client.audio.speech.create(model="gpt-4o-mini-tts", voice=voice, input=text)
        # response.stream_to_file(out_path)

        # ì¼ë¶€ SDK ë²„ì „ì—ì„œëŠ” stream_to_fileê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì‹œë„
        resp = client.audio.speech.create(
            model="gpt-4o-mini-tts",
            voice=voicec,
            input=text
        )

        # respê°€ stream_to_file ë©”ì„œë“œë¥¼ ì œê³µí•˜ë©´ ì‚¬ìš©í•˜ê³ , ì•„ë‹ˆë©´ ë°”ì´ë„ˆë¦¬ë¡œ ì €ì¥ ì‹œë„
        if hasattr(resp, "stream_to_file"):
            resp.stream_to_file(out_path)
        else:
            # respë¥¼ ì½ì–´ì„œ íŒŒì¼ë¡œ ì“°ê¸° (ëŒ€ë¶€ë¶„ì˜ ìµœì‹  SDKì—ì„œ í•„ìš” ì—†ì„ ìˆ˜ ìˆìŒ)
            # respëŠ” response-like ê°ì²´ë¼ ê°€ì •. ì•„ë˜ëŠ” ì•ˆì „í•œ fallback.
            try:
                data = resp.read()  # ì¼ë¶€ êµ¬í˜„ì—ì„œ .read()ë¡œ ë°”ì´ë„ˆë¦¬ ì–»ê¸°
            except Exception:
                # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: respë¥¼ strë¡œ ë³€í™˜í•´ì„œ ë°”ì´íŠ¸ë¡œ ì €ì¥ (ì˜ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
                data = bytes(str(resp), "utf-8")
            with open(out_path, "wb") as f:
                f.write(data)

        return out_path
    except Exception as e:
        # ì‹¤íŒ¨ ì‹œ ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
        shutil.rmtree(out_dir, ignore_errors=True)
        raise e

# -----------------------------
# Streamlit ì´ˆê¸°í™”
# -----------------------------
if "panel" not in st.session_state:
    st.session_state.panel = "voice_select"

st.sidebar.markdown("ë©”ë‰´ ì´ë™ê¸°")

if st.sidebar.button("ëª©ì†Œë¦¬ ì„ íƒìœ¼ë¡œ ì´ë™."):
    if st.session_state.panel == "voice_select":
        st.toast("ì´ë¯¸ ëª©ì†Œë¦¬ ì„ íƒì— ìˆìŠµë‹ˆë‹¤. íœ´ë¨¼.")
    else:
        change_panel(0)

if st.sidebar.button("ìŒì„± ë©”ëª¨ë¡œ ì´ë™."):
    if st.session_state.panel == "memo":
        st.toast("ì´ë¯¸ ìŒì„± ë©”ëª¨ì— ìˆìŠµë‹ˆë‹¤. íœ´ë¨¼.")
    else:
        change_panel(1)

if st.sidebar.button("ìˆ˜ì–´ ì¸ì‹ê¸°ë¡œ ì´ë™."):
    if st.session_state.panel == "hand_lang":
        st.toast("ì´ë¯¸ ìˆ˜ì–´ ì¸ì‹ê¸°ì— ìˆìŠµë‹ˆë‹¤. íœ´ë¨¼.")
    else:
        change_panel(2)


# ëª©ì†Œë¦¬ ì„ íƒ ë¼ë””ì˜¤ (ì „ì—­ í‚¤ë¡œ ì €ì¥)
if st.session_state.panel == "voice_select":
    st.title("ì›í•˜ì‹œëŠ” ëª©ì†Œë¦¬ë¥¼ ê³¨ë¼ì£¼ì„¸ìš”.")
    voice = st.radio("", options=["ì–´ë¦°ì´ ëª©ì†Œë¦¬", "ì–´ë¥¸ì´ ëª©ì†Œë¦¬", "ë…¸ì¸ì´ ëª©ì†Œë¦¬"])
    if st.button("ëŒ€í™” ì‹œì‘í•˜ê¸°"):
        if voice == "ì–´ë¦°ì´ ëª©ì†Œë¦¬":
            questions = [
                "ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë•Œ?",
                "ì¢‹ì•„í•˜ëŠ” ìŒì‹ì€ ë­ì•¼?",
                "ì˜¤ëŠ˜ ë­ í•˜ê³  ë†€ê³  ì‹¶ì–´?",
                "ì˜¤ëŠ˜ í•™êµì—ì„œ ë¬´ìŠ¨ì¼ì´ ìˆì—ˆì–´?",
                "ìµœê·¼ì— ë°°ìš´ ì¬ë°ŒëŠ” ê±´ ë­ì•¼?",
                "ì˜¤ëŠ˜ í•˜ê³  ì‹¶ì€ ê²Œì„ì´ ìˆì–´?",
                "ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ë§Œí™”ëŠ” ë­ì•¼?"
            ]
            txt = random.choice(questions)
            voice_name = "fable"
        elif voice == "ì–´ë¥¸ì´ ëª©ì†Œë¦¬":
            questions = [
                "ì˜¤ëŠ˜ ì—…ë¬´ëŠ” ì˜ ë˜ì—ˆë‚˜ìš”?",
                "ìµœê·¼ ì½ì€ ì±…ì´ ìˆë‚˜ìš”?",
                "ì£¼ë§ ê³„íšì´ ìˆë‚˜ìš”?",
                "ì˜¤ëŠ˜ ì ì‹¬ ë­ ë“œì…¨ë‚˜ìš”?",
                "ìµœê·¼ ê´€ì‹¬ ìˆëŠ” ë‰´ìŠ¤ê°€ ìˆë‚˜ìš”?"
            ]
            txt = random.choice(questions)
            voice_name = "alloy"
        elif voice == "ë…¸ì¸ì´ ëª©ì†Œë¦¬":
            questions = [
                "ì˜¤ëŠ˜ í•˜ë£¨ëŠ” ì–´ë• ë‚˜ìš”?",
                "ì˜›ë‚  ì´ì•¼ê¸°ë¥¼ í•˜ë‚˜ í•´ì£¼ì‹¤ë˜ìš”?",
                "ì¢‹ì•„í•˜ëŠ” ì·¨ë¯¸ê°€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ì Šì—ˆì„ ë•Œ ê°€ì¥ ê¸°ì–µì— ë‚¨ëŠ” ì¼ì€ ë­ì˜ˆìš”?",
                "ê±´ê°• ê´€ë¦¬ëŠ” ì˜ í•˜ê³  ê³„ì‹ ê°€ìš”?"
            ]
            txt = random.choice(questions)
            voice_name = "echo"
        
        try:
            with st.spinner("TTS ìƒì„± ì¤‘..."):
                mp3_path = text_to_speech(txt, voicec=voice_name)
            # Streamlitì—ì„œ ì¬ìƒ
            with open(mp3_path, "rb") as f:
                audio_bytes = f.read()
            st.audio(audio_bytes, format="audio/mp3")
        except:
            print("error")

# ìŒì„± ë©”ëª¨ íŒ¨ë„
elif st.session_state.panel == "memo":
    st.title("ìŒì„± ë©”ëª¨")

    # ë²„íŠ¼ - ë…¹ìŒ(ìŒì„±â†’í…ìŠ¤íŠ¸)
    if st.button("ìŒì„± ë…¹ìŒí•˜ê¸° (STT)"):
        audio_data = record_audio()
        user_text = transcribe(audio_data)
        st.write(user_text)

        

# ìˆ˜ì–´ ì¸ì‹ê¸° íŒ¨ë„ (í”Œë ˆì´ìŠ¤í™€ë”)
elif st.session_state.panel == "hand_lang":
    st.title("ìˆ˜ì–´ ì¸ì‹ê¸°")
    st.title("âœ‹ ì‚¬ë‘ì˜ ê°€ìœ„ë°”ìœ„ë³´ ê²Œì„ (Love Machine)")

    # --- ëª¨ë¸ê³¼ ë¼ë²¨ ë¶ˆëŸ¬ì˜¤ê¸° ---

    @st.cache_resource
    def load_teachable_model():
        model = load_model("keras_model.h5", compile=False)
        return model

    @st.cache_data
    def load_labels():
        with open("labels.txt", "r", encoding="utf-8") as f:
            return [line.strip() for line in f.readlines()]

    model = load_teachable_model()
    labels = load_labels()

    # --- ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ ---
    def preprocess_for_model(frame):
        """Teachable Machine ì…ë ¥ í˜•ì‹(224x224)ìœ¼ë¡œ ë³€í™˜"""
        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).convert("RGB")
        size = (224, 224)
        image = ImageOps.fit(image, size, Image.Resampling.LANCZOS)
        image_array = np.asarray(image)
        normalized_image_array = (image_array.astype(np.float32) / 127.5) - 1
        return np.expand_dims(normalized_image_array, axis=0)

    # --- ë¹„ë””ì˜¤ í”„ë ˆì„ ì²˜ë¦¬ í•¨ìˆ˜ ---
    def video_frame_callback(frame):
        img = frame.to_ndarray(format="bgr24")
        try:
            # ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
            data = preprocess_for_model(img)
            # ì˜ˆì¸¡ ìˆ˜í–‰
            prediction = model.predict(data)
            index = np.argmax(prediction)
            class_name = labels[index].strip()
            confidence = prediction[0][index]
            # ê²°ê³¼ í™”ë©´ì— í‘œì‹œ
            text = f"{class_name.split()[1]} ({confidence*100:.1f}%)"
            cv2.putText(img, text, (30, 50), cv2.FONT_HERSHEY_SIMPLEX,
                        1.2, (0, 255, 255), 3, cv2.LINE_AA)

        except Exception as e:
            cv2.putText(img, "Error", (30, 50), cv2.FONT_HERSHEY_SIMPLEX,
                        1, (0, 0, 255), 2, cv2.LINE_AA)

        return av.VideoFrame.from_ndarray(img, format="bgr24")

    # --- WebRTC ì‹¤í–‰ ---
    webrtc_streamer(
        key="hand-detect",
        video_frame_callback=video_frame_callback,
        media_stream_constraints={"video": True, "audio": False},
    )
    st.write("ì¹´ë©”ë¼ê°€ ì¼œì§€ë©´ ì† ëª¨ì–‘ì„ ìë™ìœ¼ë¡œ ì¸ì‹í•˜ê³ , Teachable Machine ëª¨ë¸ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤ âœ‹")
